{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11048520,"sourceType":"datasetVersion","datasetId":6882768},{"sourceId":11048525,"sourceType":"datasetVersion","datasetId":6882773},{"sourceId":11048533,"sourceType":"datasetVersion","datasetId":6882780}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import librosa\nimport numpy as np\n\ndef get_audio_peaks(audio_path):\n    y, sr = librosa.load(audio_path)\n    # Compute onset strength envelope\n    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n    \n    # Pick peaks from onset envelope\n    peaks = librosa.util.peak_pick(onset_env,\n                                   pre_max=3,\n                                   post_max=3,\n                                   pre_avg=3,\n                                   post_avg=3,\n                                   delta=0.2,\n                                   wait=5)\n    \n    # Convert peak indices to timestamps\n    times = librosa.frames_to_time(peaks, sr=sr)\n    times = np.round(times, 2)\n    return times\n\n\n# Example usage:\naudio_peaks = get_audio_peaks('/kaggle/input/audio-only/audio_only_ID_1.wav')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:53:41.817666Z","iopub.execute_input":"2025-03-16T12:53:41.817987Z","iopub.status.idle":"2025-03-16T12:53:41.866678Z","shell.execute_reply.started":"2025-03-16T12:53:41.817965Z","shell.execute_reply":"2025-03-16T12:53:41.866012Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"audio_peaks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:53:45.006128Z","iopub.execute_input":"2025-03-16T12:53:45.006433Z","iopub.status.idle":"2025-03-16T12:53:45.012850Z","shell.execute_reply.started":"2025-03-16T12:53:45.006411Z","shell.execute_reply":"2025-03-16T12:53:45.012003Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([ 1.65,  2.76,  4.88,  5.83,  8.1 ,  8.89, 11.33, 11.91, 14.58,\n       14.88, 17.81])"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"The above code block uses librosa to detect significant sound events (peaks) in an audio file. It loads the audio, computes an onset envelope (representing energy changes), and identifies peaks using librosa.util.peak_pick(). These peaks are converted to timestamps and rounded to two decimal places for precision. The function is useful for identifying impact sounds i.e ball hitting edges in audio files, which can later be matched with corresponding video events.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:30:56.601216Z","iopub.execute_input":"2025-03-16T12:30:56.601551Z","iopub.status.idle":"2025-03-16T12:30:56.605165Z","shell.execute_reply.started":"2025-03-16T12:30:56.601522Z","shell.execute_reply":"2025-03-16T12:30:56.604357Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def detect_frame_boundaries(frame):\n    # Convert frame to grayscale\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    \n    # Apply Canny edge detection\n    edges = cv2.Canny(gray_frame, 50, 150)\n    \n    # Find contours in the edge-detected image\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    for contour in contours:\n        # Approximate contour to a polygon\n        approx = cv2.approxPolyDP(contour, 0.01 * cv2.arcLength(contour, True), True)\n        \n        # Check if it is a rectangle (4 corners)\n        if len(approx) == 4:\n            x, y, w, h = cv2.boundingRect(approx)  # Get bounding box of rectangle\n            return x, y, w, h  # Return rectangle boundaries\n    \n    return None  # Return None if no rectangle is found\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:36:58.485366Z","iopub.execute_input":"2025-03-16T12:36:58.485731Z","iopub.status.idle":"2025-03-16T12:36:58.490973Z","shell.execute_reply.started":"2025-03-16T12:36:58.485703Z","shell.execute_reply":"2025-03-16T12:36:58.489998Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"This function is for detecting the actual boundaries of the rectangular frame in videos where collisions occur. These boundaries are used to check if the ball hits any edge during its motion.","metadata":{}},{"cell_type":"code","source":"def detect_collisions_with_frame(video_path):\n    cap = cv2.VideoCapture(video_path)\n    \n    if not cap.isOpened():\n        print(f\"Error: Unable to open video file at {video_path}\")\n        return []\n    \n    impacts = []  # List to store timestamps of collisions\n    fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n    frame_count = 0\n    \n    rect_boundaries = None  # To store detected rectangular frame boundaries\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Detect rectangular frame in the first frame\n        if rect_boundaries is None:\n            rect_boundaries = detect_frame_boundaries(frame)\n            if rect_boundaries is None:\n                rect_boundaries = [50, 34, 341, 223]\n                break\n        \n        x_rect, y_rect, w_rect, h_rect = rect_boundaries\n        \n        # Convert frame to HSV for better color segmentation\n        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n        \n        # Define HSV range for detecting the blue ball\n        lower_blue = np.array([100, 150, 50])  # Adjust these values if needed\n        upper_blue = np.array([140, 255, 255])\n        \n        # Create a mask for the blue ball\n        mask = cv2.inRange(hsv_frame, lower_blue, upper_blue)\n        \n        # Find contours in the mask\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        if contours:\n            # Select the largest contour (assumed to be the ball)\n            largest_contour = max(contours, key=cv2.contourArea)\n            x_ball, y_ball, w_ball, h_ball = cv2.boundingRect(largest_contour)  # Get bounding box\n            \n            # Calculate center of the ball\n            ball_center_x = x_ball + w_ball // 2\n            ball_center_y = y_ball + h_ball // 2\n            \n            # Check for collisions with edges of the rectangular frame\n            if (\n                ball_center_x - 6 <= x_rect or ball_center_x + 6 >= x_rect + w_rect or  # Left or right edge of rectangle\n                ball_center_y - 6 <= y_rect     # Bottom edge of rectangle\n            ):\n                impact_time = round(frame_count / fps, 2)  # Calculate timestamp in seconds and round to 2 decimal places\n                \n                # Add timestamp only if it's at least 0.25 seconds apart from the last recorded impact\n                if len(impacts) == 0 or (impact_time - impacts[-1] >= 0.25):\n                    impacts.append(impact_time)\n        \n        frame_count += 1\n    \n    cap.release()\n    return impacts\n\n# Example usage:\nvideo_path = '/kaggle/input/video-only/video_only_ID_1.mp4'\ncollisions = detect_collisions_with_frame(video_path)\nprint(\"Collision timestamps:\", collisions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:12:41.143980Z","iopub.execute_input":"2025-03-16T13:12:41.144345Z","iopub.status.idle":"2025-03-16T13:12:42.388088Z","shell.execute_reply.started":"2025-03-16T13:12:41.144317Z","shell.execute_reply":"2025-03-16T13:12:42.387195Z"}},"outputs":[{"name":"stdout","text":"Collision timestamps: [0.32, 1.65, 2.98, 4.32, 5.65, 6.98, 8.32, 9.65, 10.98, 12.32, 13.65, 14.98, 16.32, 17.65, 18.98]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"The provided code, `detect_collisions_with_frame`, identifies timestamps when a ball collides with the edges of a rectangular frame in a video. Here's a concise explanation:\n\n1. **Video Processing**: The function reads the video frame-by-frame using OpenCV. If the rectangular frame boundaries are not detected in the first frame, default boundaries (`[50, 34, 341, 223]`) are used.\n\n2. **Ball Detection**: Each frame is converted to HSV color space to isolate the blue ball using a predefined HSV range. A binary mask is created, and the largest contour in the mask is assumed to be the ball.\n\n3. **Collision Detection**: The center of the ball is calculated, and its position is checked against the rectangular frame's boundaries (left, right, and bottom edges). If a collision is detected, the timestamp (in seconds) is calculated and added to the `impacts` list, ensuring at least 0.25 seconds between consecutive timestamps.\nHere i also added a padding of 6px to the center of the ball because exact center of ball will never hit the edge.When i didnt add that padding i got no collisions and 6 number was optimal\n4. **Output**: The function returns a list of rounded timestamps (`impacts`) indicating when collisions occurred.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:58:59.079048Z","iopub.execute_input":"2025-03-16T12:58:59.079374Z","iopub.status.idle":"2025-03-16T12:58:59.505858Z","shell.execute_reply.started":"2025-03-16T12:58:59.079353Z","shell.execute_reply":"2025-03-16T12:58:59.504688Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/soln-map/submit_solution_mapping.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:58:59.854496Z","iopub.execute_input":"2025-03-16T12:58:59.854952Z","iopub.status.idle":"2025-03-16T12:58:59.870717Z","shell.execute_reply.started":"2025-03-16T12:58:59.854927Z","shell.execute_reply":"2025-03-16T12:58:59.870081Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:59:19.353721Z","iopub.execute_input":"2025-03-16T12:59:19.354072Z","iopub.status.idle":"2025-03-16T12:59:19.373229Z","shell.execute_reply.started":"2025-03-16T12:59:19.354048Z","shell.execute_reply":"2025-03-16T12:59:19.372083Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 45 entries, 0 to 44\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   AUDIO   45 non-null     object \n 1   VIDEO   0 non-null      float64\ndtypes: float64(1), object(1)\nmemory usage: 848.0+ bytes\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:59:45.882389Z","iopub.execute_input":"2025-03-16T12:59:45.882697Z","iopub.status.idle":"2025-03-16T12:59:45.891585Z","shell.execute_reply.started":"2025-03-16T12:59:45.882675Z","shell.execute_reply":"2025-03-16T12:59:45.890881Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                  AUDIO  VIDEO\n0   audio_only_ID_6.wav    NaN\n1  audio_only_ID_34.wav    NaN\n2  audio_only_ID_27.wav    NaN\n3  audio_only_ID_29.wav    NaN\n4  audio_only_ID_41.wav    NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AUDIO</th>\n      <th>VIDEO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>audio_only_ID_6.wav</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>audio_only_ID_34.wav</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>audio_only_ID_27.wav</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>audio_only_ID_29.wav</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>audio_only_ID_41.wav</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import os\n\n# Function to generate audio timestamps for all audio files\ndef generate_audio_timestamps(audio_folder):\n    audio_timestamps = {}\n    for audio_file in os.listdir(audio_folder):\n        if audio_file.endswith('.wav'):\n            # Replace this with your function to extract audio timestamps\n            timestamps = get_audio_peaks(os.path.join(audio_folder, audio_file))\n            audio_timestamps[audio_file] = timestamps\n    return audio_timestamps\n\n# Function to generate video timestamps for all video files\ndef generate_video_timestamps(video_folder):\n    video_timestamps = {}\n    for video_file in os.listdir(video_folder):\n        if video_file.endswith('.mp4'):\n            # Replace this with your function to extract video timestamps\n            timestamps = detect_collisions_with_frame(os.path.join(video_folder, video_file))\n            video_timestamps[video_file] = timestamps\n    return video_timestamps\n\n# Example usage\naudio_folder = '/kaggle/input/audio-only'\nvideo_folder = '/kaggle/input/video-only'\n\naudio_timestamps_dict = generate_audio_timestamps(audio_folder)\nvideo_timestamps_dict = generate_video_timestamps(video_folder)\n\nprint(len(audio_timestamps_dict))\nprint(len(video_timestamps_dict))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:13:52.447662Z","iopub.execute_input":"2025-03-16T13:13:52.448029Z","iopub.status.idle":"2025-03-16T13:14:39.174602Z","shell.execute_reply.started":"2025-03-16T13:13:52.448002Z","shell.execute_reply":"2025-03-16T13:14:39.173698Z"}},"outputs":[{"name":"stdout","text":"45\n45\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Here we are generating all the time stamps for both audio and video files","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_matching_cost(audio_timestamps, video_timestamps):\n    \"\"\"\n    Compute the cost of matching an audio file with a video file.\n    The cost is defined as the sum of absolute differences between closest timestamps.\n    \"\"\"\n    if not video_timestamps:  # Handle empty list case\n        return float('inf')  # Assign a high cost when no match is possible\n\n    cost = sum(abs(min(video_timestamps, key=lambda x: abs(x - a_time)) - a_time) for a_time in audio_timestamps)\n    return cost\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:31:53.588943Z","iopub.execute_input":"2025-03-16T13:31:53.589290Z","iopub.status.idle":"2025-03-16T13:31:53.593995Z","shell.execute_reply.started":"2025-03-16T13:31:53.589267Z","shell.execute_reply":"2025-03-16T13:31:53.593081Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def one_to_one_matching(audio_timestamps_dict, video_timestamps_dict):\n    \"\"\"\n    Perform 1-to-1 matching between audio files and video files based on timestamp similarity.\n    Returns a dictionary where keys are audio filenames and values are matched video filenames.\n    \"\"\"\n    # Create a list of all possible pairs (audio, video) with their costs\n    pairs = []\n    for audio_file, audio_timestamps in audio_timestamps_dict.items():\n        for video_file, video_timestamps in video_timestamps_dict.items():\n            cost = compute_matching_cost(audio_timestamps, video_timestamps)\n            pairs.append((audio_file, video_file, cost))\n    \n    # Sort pairs by cost (ascending order)\n    pairs.sort(key=lambda x: x[2])\n    \n    # Perform greedy matching\n    matched_audio = set()\n    matched_video = set()\n    matches = {}\n\n    for audio_file, video_file, cost in pairs:\n        if audio_file not in matched_audio and video_file not in matched_video:\n            matches[audio_file] = video_file\n            matched_audio.add(audio_file)\n            matched_video.add(video_file)\n    \n    return matches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:28:24.627426Z","iopub.execute_input":"2025-03-16T13:28:24.627721Z","iopub.status.idle":"2025-03-16T13:28:24.632952Z","shell.execute_reply.started":"2025-03-16T13:28:24.627701Z","shell.execute_reply":"2025-03-16T13:28:24.632196Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import pandas as pd\n\ndef update_dataframe_with_matches(df, matches_dict):\n    \"\"\"\n    Update the VIDEO column of the DataFrame with matched video filenames.\n    \"\"\"\n    for index, row in df.iterrows():\n        audio_file = row['AUDIO']\n        if audio_file in matches_dict:\n            df.at[index, 'VIDEO'] = matches_dict[audio_file]\n    \n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:28:58.578982Z","iopub.execute_input":"2025-03-16T13:28:58.579322Z","iopub.status.idle":"2025-03-16T13:28:58.583503Z","shell.execute_reply.started":"2025-03-16T13:28:58.579296Z","shell.execute_reply":"2025-03-16T13:28:58.582731Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\nmatches_dict = one_to_one_matching(audio_timestamps_dict, video_timestamps_dict)\n\n# Update DataFrame with matches\ndf = update_dataframe_with_matches(df, matches_dict)\n\n# Save updated DataFrame to CSV\ndf.to_csv('audio_video_matches.csv', index=False)\n\nprint(\"Updated DataFrame:\")\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:03.393469Z","iopub.execute_input":"2025-03-16T13:32:03.393828Z","iopub.status.idle":"2025-03-16T13:32:03.523257Z","shell.execute_reply.started":"2025-03-16T13:32:03.393772Z","shell.execute_reply":"2025-03-16T13:32:03.522499Z"}},"outputs":[{"name":"stdout","text":"Updated DataFrame:\n                   AUDIO                 VIDEO\n0    audio_only_ID_6.wav  video_only_ID_32.mp4\n1   audio_only_ID_34.wav  video_only_ID_27.mp4\n2   audio_only_ID_27.wav  video_only_ID_40.mp4\n3   audio_only_ID_29.wav  video_only_ID_10.mp4\n4   audio_only_ID_41.wav  video_only_ID_37.mp4\n5   audio_only_ID_13.wav  video_only_ID_14.mp4\n6   audio_only_ID_32.wav  video_only_ID_41.mp4\n7   audio_only_ID_45.wav  video_only_ID_25.mp4\n8   audio_only_ID_36.wav  video_only_ID_13.mp4\n9   audio_only_ID_44.wav  video_only_ID_36.mp4\n10  audio_only_ID_40.wav   video_only_ID_5.mp4\n11   audio_only_ID_9.wav  video_only_ID_17.mp4\n12   audio_only_ID_2.wav  video_only_ID_16.mp4\n13  audio_only_ID_30.wav  video_only_ID_28.mp4\n14  audio_only_ID_19.wav  video_only_ID_35.mp4\n15  audio_only_ID_20.wav  video_only_ID_39.mp4\n16  audio_only_ID_43.wav  video_only_ID_21.mp4\n17  audio_only_ID_11.wav   video_only_ID_7.mp4\n18   audio_only_ID_7.wav   video_only_ID_2.mp4\n19  audio_only_ID_23.wav   video_only_ID_6.mp4\n20  audio_only_ID_18.wav   video_only_ID_4.mp4\n21  audio_only_ID_14.wav  video_only_ID_12.mp4\n22  audio_only_ID_17.wav  video_only_ID_29.mp4\n23  audio_only_ID_39.wav  video_only_ID_19.mp4\n24  audio_only_ID_10.wav  video_only_ID_42.mp4\n25  audio_only_ID_22.wav  video_only_ID_20.mp4\n26  audio_only_ID_15.wav  video_only_ID_18.mp4\n27   audio_only_ID_4.wav  video_only_ID_43.mp4\n28  audio_only_ID_37.wav   video_only_ID_1.mp4\n29   audio_only_ID_5.wav  video_only_ID_38.mp4\n30   audio_only_ID_3.wav  video_only_ID_24.mp4\n31  audio_only_ID_35.wav  video_only_ID_22.mp4\n32  audio_only_ID_38.wav   video_only_ID_8.mp4\n33  audio_only_ID_21.wav  video_only_ID_34.mp4\n34   audio_only_ID_8.wav  video_only_ID_23.mp4\n35  audio_only_ID_42.wav   video_only_ID_9.mp4\n36  audio_only_ID_25.wav   video_only_ID_3.mp4\n37  audio_only_ID_28.wav  video_only_ID_45.mp4\n38  audio_only_ID_12.wav  video_only_ID_33.mp4\n39  audio_only_ID_16.wav  video_only_ID_11.mp4\n40  audio_only_ID_26.wav  video_only_ID_30.mp4\n41   audio_only_ID_1.wav  video_only_ID_15.mp4\n42  audio_only_ID_31.wav  video_only_ID_26.mp4\n43  audio_only_ID_24.wav  video_only_ID_44.mp4\n44  audio_only_ID_33.wav  video_only_ID_31.mp4\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"output_file_path = '/kaggle/working/audio_video_matches.csv'  # Path to save the file\n\n# Save the DataFrame to a CSV file\ndf.to_csv(output_file_path, index=False)\n\nprint(f\"DataFrame saved successfully to {output_file_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:35:04.353195Z","iopub.execute_input":"2025-03-16T13:35:04.353531Z","iopub.status.idle":"2025-03-16T13:35:04.359912Z","shell.execute_reply.started":"2025-03-16T13:35:04.353509Z","shell.execute_reply":"2025-03-16T13:35:04.359015Z"}},"outputs":[{"name":"stdout","text":"DataFrame saved successfully to /kaggle/working/audio_video_matches.csv.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}